{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGP 5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. import files into dataframe\n",
    "2. extract 'full' days (1440 rows per date)\n",
    "3. extract number of days matching scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions in python file with magic command\n",
    "%run ../code/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "folderpath = '../depresjon'\n",
    "output_csv_path = '../output/'\n",
    "scores_csv_path = '../depresjon/scores.csv'\n",
    "\n",
    "# extract files\n",
    "df = extract_from_folder(folderpath)\n",
    "\n",
    "# extract full days (true days)\n",
    "full_df = preprocess_full_days(df)\n",
    "\n",
    "# extract days per scores \n",
    "final = extract_days_per_scores(full_df, scores_csv_path)\n",
    "\n",
    "# pivot df to wide format\n",
    "final_pivot = pivot_dataframe(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "final_pivot.to_csv(output_csv_path + 'preprocessed-wide.csv', index=False)\n",
    "final.to_csv(output_csv_path+ 'preprocessed-long.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variable names to delete\n",
    "var_list = ['df', 'full_df',  'final', 'final_pivot']\n",
    "\n",
    "# loop over the list and delete variables if they exist\n",
    "for var in var_list:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Kept all id, date combinations to maximise data\n",
    "* will split into train, test, val\n",
    "* will keep proportions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from CSV\n",
    "\n",
    "1. import preprocessed csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_csv_path = '../output/'\n",
    "scores_csv_path = '../depresjon/scores.csv'\n",
    "\n",
    "# import from csv\n",
    "df = pd.read_csv(output_csv_path + 'preprocessed-long.csv', parse_dates=['timestamp', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    ">all row level, therfore no data leakage - that is features are computed separately for each (id, date) combination so that there is no data leakage / contamination\n",
    "\n",
    "\n",
    "* **inactiveDay**: The proportion of time during the day when the participant is inactive.\n",
    "\n",
    "\\[\n",
    "\\text{{inactiveDay}} = \\frac{{\\text{{Number of inactive hours during the day}}}}{{\\text{{Total number of hours during the day}}}}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **activeNight**: The proportion of time during the night when the participant is active.\n",
    "\n",
    "\\[\n",
    "\\text{{activeNight}} = \\frac{{\\text{{Number of active hours during the night}}}}{{\\text{{Total number of hours during the night}}}}\n",
    "\\]\n",
    "\n",
    "* **inactiveLight**: The proportion of time during periods of light (e.g., daytime) when the participant is inactive.\n",
    "\n",
    "\\[\n",
    "\\text{{inactiveLight}} = \\frac{{\\text{{Number of inactive hours during periods of light}}}}{{\\text{{Total number of hours during periods of light}}}}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **activeDark**: The proportion of time during periods of darkness (e.g., nighttime) when the participant is active.\n",
    "\n",
    "\\[\n",
    "\\text{{activeDark}} = \\frac{{\\text{{Number of active hours during periods of darkness}}}}{{\\text{{Total number of hours during periods of darkness}}}}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **mean**: The average value of activity data for each hour of the day. It represents the central tendency of the data.\n",
    "\n",
    "\\[\n",
    "\\text{{mean}}_{\\text{{person-date}}} = \\frac{{\\sum_{i=1}^{n} \\text{{activity}}_{\\text{{person-date}}}(i)}}{{n}}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **std**: The standard deviation of activity data for each hour of the day. It measures the dispersion or spread of the data around the mean.\n",
    "\n",
    "\\[\n",
    "\\text{{std}}_{\\text{{person-date}}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{{activity}}_{\\text{{person-date}}}(i) - \\text{{mean}}_{\\text{{person-date}}})^2}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **percentZero**: The percentage of data points that have a value of zero for each hour of the day.\n",
    "\n",
    "\\[\n",
    "\\text{{percent\\_zero}}_{\\text{{person-date}}} = \\frac{{\\text{{Number of hours with zero activity}}_{\\text{{person-date}}}}}{{\\text{{Total number of hours}}_{\\text{{person-date}}}}} \\times 100\n",
    "\\]\n",
    "\n",
    "\n",
    "* **kurtosis**: A measure of the \"tailedness\" or shape of a distribution. It indicates how sharply peaked or flat the distribution is compared to a normal distribution. Positive kurtosis indicates a relatively peaked distribution, while negative kurtosis indicates a relatively flat distribution.\n",
    "\n",
    "\\[\n",
    "\\text{{kurtosis}}_{\\text{{person-date}}} = \\frac{{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{{activity}}_{\\text{{person-date}}}(i) - \\text{{mean}}_{\\text{{person-date}}})^4}}{{\\left( \\frac{1}{n} \\sum_{i=1}^{n} (\\text\n",
    "\n",
    "\n",
    "\n",
    "To calculate the above features: \n",
    "\n",
    "* **Day / Night** - determined by hours, e.g. 08:00-20:00\n",
    "\n",
    "\\[\n",
    "\\text{{day\\_night}} = \\begin{cases} \n",
    "0 & \\text{{if }} \\text{{day\\_start}} \\leq \\text{{hour}} < \\text{{day\\_end}} \\\\\n",
    "1 & \\text{{otherwise}}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "* **Light / Dark** - determined by monthly sunset/sunrise times in Norway\n",
    "\n",
    "\\[\n",
    "\\text{{light\\_dark}} = \\begin{cases} \n",
    "0 & \\text{{if }} \\text{{sunrise\\_time}} \\leq \\text{{timestamp}} < \\text{{sunset\\_time}} \\\\\n",
    "1 & \\text{{otherwise}}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "\n",
    "* **Active / Inactive** - active is where the rolling average (window = 11) of 'active minute' (threshold > 5) is greater than 2\n",
    "\n",
    "\\[\n",
    "\\text{{active\\_inactive}} = \\begin{cases} \n",
    "1 & \\text{{if }} \\text{{activity}} \\geq \\text{{activity\\_threshold}} \\\\\n",
    "0 & \\text{{otherwise}}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{{rolling\\_sum}} = \\text{{rolling sum of }} \\text{{active\\_inactive}} \\text{{ over a window of }} \\text{{rolling\\_window}}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{{active\\_inactive\\_period}} = \\begin{cases} \n",
    "1 & \\text{{if }} \\text{{rolling\\_sum}} \\geq \\text{{rolling\\_threshold}} \\\\\n",
    "0 & \\text{{otherwise}}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions in python file with magic command\n",
    "%run ../code/features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features\n",
    "features_full = calculate_all_features(df, sunlight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Female, Male, Both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/IGP-5-public/code/model.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSVC\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscriminant_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuadraticDiscriminantAnalysis\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# load functions in python file with magic command\n",
    "%run ../code/model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male dataset shape: (310, 9)\n",
      "Female dataset shape: (383, 9)\n",
      "Both genders dataset shape: (693, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "male, female, both = split_and_prepare_data(features_full)\n",
    "\n",
    "# shapes of the datasets \n",
    "print(f\"Male dataset shape: {male.shape}\")\n",
    "print(f\"Female dataset shape: {female.shape}\")\n",
    "print(f\"Both genders dataset shape: {both.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  inactiveDay  activeNight  inactiveLight  activeDark        mean  \\\n",
      "0      1     0.144444     0.300000       0.300193    0.264851  156.247222   \n",
      "1      1     0.301389     0.354167       0.395753    0.326733  124.135417   \n",
      "2      1     0.125000     0.341667       0.301158    0.376238  134.961806   \n",
      "3      1     0.144444     0.269444       0.291506    0.188119   99.439583   \n",
      "4      1     0.111111     0.301389       0.271236    0.252475  316.874306   \n",
      "\n",
      "          std  percent_zero   kurtosis  \n",
      "0  229.109777     40.902778   8.792571  \n",
      "1  211.241278     46.180556  10.550960  \n",
      "2  230.954732     37.430556  15.449014  \n",
      "3  177.719972     42.013889  21.223210  \n",
      "4  496.184847     39.375000   7.679689  \n"
     ]
    }
   ],
   "source": [
    "print(male.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "X_train, X_validation, y_train, y_validation = validation_data(male)\n",
    "\n",
    "# evaluate models\n",
    "results = evaluate_models(models1, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models for training time (fastest to slowest):\n",
      "1. Naive Bayes: 0.018370437622070312 seconds\n",
      "2. Decision Tree: 0.022133302688598634 seconds\n",
      "3. SVC linear: 0.0240386962890625 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print_top_models(results, metric='accuracy')\n",
    "print_top_models(results, top_n=3)\n",
    "#print_top_models(results, metric='mcc', top_n=10)\n",
    "#print_top_models(results, metric='f1', top_n=10)\n",
    "#print_top_models(results, metric='training_time', top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP and VIF\n",
    "\n",
    "SHAP (SHapley Additive exPlanations):\n",
    "SHAP is a method based on cooperative game theory that explains the output of any machine learning model by computing the contribution of each feature to the prediction.\n",
    "It provides a unified approach to interpret the output of any model, including complex models like ensemble methods and deep learning models.\n",
    "SHAP values represent the impact of each feature on the model's output for a particular instance. Positive SHAP values indicate features that contribute to increasing the prediction, while negative values indicate features that decrease the prediction.\n",
    "By analyzing SHAP values, you can identify which features have the most significant impact on model predictions and understand the relationship between features and predictions.\n",
    "VIF (Variance Inflation Factor):\n",
    "VIF is a measure used to quantify the severity of multicollinearity in a regression analysis.\n",
    "Multicollinearity occurs when two or more features in a regression model are highly correlated, which can lead to unreliable estimates of the regression coefficients.\n",
    "VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated.\n",
    "A high VIF value (typically greater than 5 or 10) indicates that multicollinearity may be problematic, and the corresponding feature may be redundant or highly correlated with other features.\n",
    "By calculating VIF for each feature, you can identify features that exhibit multicollinearity and assess whether they should be retained in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def assess_feature_importance(models, X_train, y_train, shap_sampling='auto'):\n",
    "    \"\"\"\n",
    "    Assess feature importance and multicollinearity for a list of models.\n",
    "    \n",
    "    Parameters:\n",
    "    - models: A list of tuples containing (model_name, model_instance) pairs.\n",
    "    - X_train: Training features.\n",
    "    - y_train: Training target variable.\n",
    "    - shap_sampling: The method used for SHAP sampling. Defaults to 'auto' (use TreeExplainer for tree-based models, otherwise KernelExplainer).\n",
    "\n",
    "    Returns:\n",
    "    - feature_importance_df: DataFrame containing feature importance scores for each model.\n",
    "    - vif_df: DataFrame containing VIF scores for each feature.\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    vif = {}\n",
    "\n",
    "    # Calculate SHAP values for each model\n",
    "    for model_name, model in models:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            explainer = shap.TreeExplainer(model, data=X_train)\n",
    "            shap_values = explainer.shap_values(X_train, approximate=True)\n",
    "        else:\n",
    "            if shap_sampling == 'auto':\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train)\n",
    "            elif shap_sampling == 'fast':\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train.iloc[:1000, :])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid shap_sampling parameter. Choose 'auto' or 'fast'.\")\n",
    "            shap_values = explainer.shap_values(X_train, nsamples=100)\n",
    "        \n",
    "        # Calculate SHAP feature importance\n",
    "        feature_importance[model_name] = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Convert SHAP feature importance to DataFrame\n",
    "    feature_importance_df = pd.DataFrame(feature_importance, index=X_train.columns)\n",
    "    \n",
    "    # Calculate VIF for each feature\n",
    "    for i, feature in enumerate(X_train.columns):\n",
    "        vif[feature] = variance_inflation_factor(X_train.values, i)\n",
    "\n",
    "    # Convert VIF to DataFrame\n",
    "    vif_df = pd.DataFrame(vif, index=['VIF']).T\n",
    "\n",
    "    return feature_importance_df, vif_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(feature_importance_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance_df.plot(kind='bar', figsize=(12, 8))\n",
    "    plt.title('Feature Importance Scores')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Models')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_feature_importance(feature_importance_df)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_feature_importance_heatmap(feature_importance_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(feature_importance_df.T, cmap='viridis', annot=True, fmt=\".3f\")\n",
    "    plt.title('Feature Importance Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Models')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_feature_importance_heatmap(feature_importance_df)\n",
    "\n",
    "\n",
    "def plot_vif(vif_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    vif_df['VIF'].plot(kind='bar', figsize=(12, 8))\n",
    "    plt.title('Variance Inflation Factor (VIF)')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('VIF Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_vif(vif_df)\n",
    "\n",
    "def plot_vif_heatmap(vif_df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(vif_df.T, cmap='viridis', annot=True, fmt=\".3f\")\n",
    "    plt.title('VIF Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Models')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_vif_heatmap(vif_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model selection and evaluation strategy\n",
    "  * either start with many models (garcia) - no hyperparameter\n",
    "  * choose best mcc, f1, accuracy -> top 3 to go into next round\n",
    "  * then look at feature importance -> rationale\n",
    "  * then look at hyperparameter tuning final model\n",
    "  * then look at ensemble??\n",
    "  * repeat for other datasets\n",
    " \n",
    "* model evaluation\n",
    "* metric selection and reason\n",
    "  * `accuracy` - prop of correct predictions; good overall performanced indicator\n",
    "  * `recall (sensitivity)` - prop of actual positives that are correctly identified.  ability to identify all actual cases of depression.  crucial to minimise false negatives that is failing to identify individuals who are depressed.\n",
    "  * `precision` - prop of predicted depression which are correct (true positive predictions among all positive predictions) - important when need to avoid false positives (unnecessary concern, intervention, medication, treatment)\n",
    "   * `F1` - harmonic mean of precision and recall - balance between the two, especially if imbalanced class distribution\n",
    "   * `specificity` - ability to identify non-depression correctly - important to ensure healthy individuals are not misclassified -  measures the proportion of actual negatives that are correctly identified by the mode\n",
    "  * `MCC` - takes into account true adn false positives and negatives.  reliable statistic rate that produces a high score only if the prediction obtained good results in all four matrix categories\n",
    "  * `ROC-AUC - Area Under the Receiver Operating Characteristic Curve`: Evaluates the model’s ability to discriminate between the classes. A higher AUC indicates better model performance.   ROC-AUC is suitable for depression prediction when you want to evaluate the model's ability to distinguish between depressed and non-depressed individuals across different threshold settings.\n",
    "  * `training time`\n",
    "\n",
    "TODO research Matthews Correlation Coefficient, F1 as key metrics - getting the balance right\n",
    "TODO add metric maths to slides and their importance (contextual)\n",
    "\n",
    "\n",
    "\n",
    "* feature importance analysis - SHAP, Feature Permutation\n",
    "* Hyperparameter tuning\n",
    "* Ensemble models\n",
    "* Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flexible Decision Boundary: Unlike linear classifiers like Logistic Regression or Linear Discriminant Analysis (LDA), QDA can model non-linear decision boundaries between classes. This flexibility allows QDA to capture more complex relationships in the data.\n",
    "Unrestricted Covariance Matrices: QDA allows each class to have its own covariance matrix, whereas Linear Discriminant Analysis (LDA) assumes a common covariance matrix for all classes. This can be beneficial when the classes have different variances or when the relationship between features and classes is complex.\n",
    "Handling Non-Normal Data: Although QDA assumes that the data within each class follows a multivariate normal distribution, it can still perform well even if this assumption is not strictly met, especially if the departure from normality is not severe.\n",
    "Effective with Small Datasets: QDA can be effective with small datasets because it estimates separate covariance matrices for each class, potentially providing better modeling of the underlying data distribution.\n",
    "Robustness to Outliers: QDA can be more robust to outliers compared to linear classifiers like Logistic Regression because it models each class's covariance separately, allowing it to better adapt to the data distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igp5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
